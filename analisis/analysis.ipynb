{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An\u00e1lisis de Din\u00e1micas de Tr\u00e1fico (Semana Aleatoria)\n",
    "\n",
    "Este notebook analiza los patrones de tr\u00e1fico de diferentes servicios (SMS, Llamadas, Internet). \n",
    "**Enfoque:** Se selecciona **una semana aleatoria** del conjunto de datos (de Lunes a Domingo) para analizar los patrones temporales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Configuraci\u00f3n de visualizaci\u00f3n\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['animation.embed_limit'] = 100  # Aumentar l\u00edmite a 100MB para permitir animaciones largas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y Procesamiento Temporal\n",
    "\n",
    "Cargamos los datos utilizando Dask para eficiencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['../data1.csv/data1.csv', '../data2.csv/data2.csv']\n",
    "services = ['smsin', 'smsout', 'callin', 'callout', 'internet']\n",
    "\n",
    "valid_files = [f for f in files if os.path.exists(f)]\n",
    "if not valid_files:\n",
    "    raise FileNotFoundError(\"No se encontraron los archivos de datos.\")\n",
    "\n",
    "print(f\"Cargando archivos: {valid_files}\")\n",
    "\n",
    "# Leer CSVs con Dask\n",
    "ddf = dd.read_csv(valid_files, assume_missing=True)\n",
    "\n",
    "# --- AGREGACI\u00d3N TEMPORAL ---\n",
    "ddf_time = ddf[['TimeInterval'] + services]\n",
    "agg_task = ddf_time.groupby('TimeInterval')[services].sum()\n",
    "\n",
    "print(\"Procesando datos temporales...\")\n",
    "with ProgressBar():\n",
    "    final_df = agg_task.compute()\n",
    "\n",
    "# Post-procesamiento\n",
    "final_df = final_df.reset_index()\n",
    "final_df['Timestamp'] = pd.to_datetime(final_df['TimeInterval'], unit='ms')\n",
    "final_df = final_df.sort_values('Timestamp')\n",
    "final_df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "print(\"Rango de fechas total disponible:\")\n",
    "print(f\"Inicio: {final_df.index.min()}\")\n",
    "print(f\"Fin:    {final_df.index.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecci\u00f3n de Semana Aleatoria (Lunes - Domingo)\n",
    "\n",
    "Seleccionamos una semana al azar dentro del rango de fechas disponible, asegurando que comience en **Lunes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rango total de fechas\n",
    "min_date = final_df.index.min()\n",
    "max_date = final_df.index.max()\n",
    "\n",
    "# Encontrar el primer lunes disponible\n",
    "days_to_first_monday = (7 - min_date.weekday()) % 7\n",
    "first_monday = min_date + pd.Timedelta(days=days_to_first_monday)\n",
    "first_monday = first_monday.normalize()\n",
    "\n",
    "# Calcular cu\u00e1ntas semanas completas hay\n",
    "total_days = (max_date - first_monday).days\n",
    "total_weeks = total_days // 7\n",
    "\n",
    "if total_weeks < 1:\n",
    "    print(\"Advertencia: Menos de una semana completa de datos desde el primer lunes. Usando la primera semana disponible ajustada.\")\n",
    "    start_date = first_monday\n",
    "else:\n",
    "    # Elegir una semana aleatoria\n",
    "    random_week_offset = random.randint(0, total_weeks - 1)\n",
    "    start_date = first_monday + pd.Timedelta(weeks=random_week_offset)\n",
    "\n",
    "end_date = start_date + pd.Timedelta(days=7)\n",
    "\n",
    "# Filtrar el DataFrame\n",
    "week_df = final_df[(final_df.index >= start_date) & (final_df.index < end_date)]\n",
    "\n",
    "print(f\"\\nAnalizando semana aleatoria: del {start_date.date()} (Lunes) al {end_date.date()} (Domingo)\")\n",
    "display(week_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci\u00f3n Semanal\n",
    "\n",
    "Gr\u00e1ficas detalladas del comportamiento del tr\u00e1fico durante la semana seleccionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True)\n",
    "\n",
    "# Gr\u00e1fico para Internet\n",
    "ax1.plot(week_df.index, week_df['internet'], color='purple', label='Internet', linewidth=2)\n",
    "ax1.set_title(f'Tr\u00e1fico de Internet (Semana del {start_date.date()})', fontsize=14)\n",
    "ax1.set_ylabel('Volumen de Datos')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gr\u00e1fico para SMS y Llamadas\n",
    "colors = {'smsin': 'green', 'smsout': 'lightgreen', 'callin': 'blue', 'callout': 'cyan'}\n",
    "for service in ['smsin', 'smsout', 'callin', 'callout']:\n",
    "    ax2.plot(week_df.index, week_df[service], label=service, color=colors.get(service), alpha=0.8)\n",
    "\n",
    "ax2.set_title(f'Tr\u00e1fico de SMS y Llamadas (Semana del {start_date.date()})', fontsize=14)\n",
    "ax2.set_xlabel('Tiempo')\n",
    "ax2.set_ylabel('Volumen de Eventos')\n",
    "ax2.legend(loc='upper right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Forzar l\u00edmites del eje X\n",
    "ax2.set_xlim(start_date, end_date)\n",
    "\n",
    "# Formatear eje X\n",
    "import matplotlib.dates as mdates\n",
    "ax2.xaxis.set_major_formatter(mdates.DateFormatter('%A %H:%M'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../imagenes/internet_sms_call_traffic.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribuci\u00f3n Espacial 3D (Est\u00e1tica)\n",
    "\n",
    "Visualizaci\u00f3n en 3D de la distribuci\u00f3n del tr\u00e1fico acumulado en la cuadr\u00edcula urbana (100x100). La altura (eje Z) representa el volumen de tr\u00e1fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AGREGACI\u00d3N ESPACIAL ---\n",
    "print(\"Procesando distribuci\u00f3n espacial...\")\n",
    "ddf_spatial = ddf[['GridID'] + services]\n",
    "spatial_task = ddf_spatial.groupby('GridID')[services].sum()\n",
    "\n",
    "with ProgressBar():\n",
    "    spatial_df = spatial_task.compute()\n",
    "\n",
    "spatial_df['sms_total'] = spatial_df['smsin'] + spatial_df['smsout']\n",
    "spatial_df['call_total'] = spatial_df['callin'] + spatial_df['callout']\n",
    "\n",
    "# Funci\u00f3n auxiliar para plotear 3D\n",
    "def plot_3d_surface(data_series, title, filename, cmap='inferno'):\n",
    "    # --- CREACI\u00d3N DE LA MATRIZ ---\n",
    "    grid_matrix = np.zeros((100, 100))\n",
    "    \n",
    "    # Mapeo GridID -> Matriz\n",
    "    # Asumiendo que el \u00edndice de data_series es GridID\n",
    "    for grid_id, val in data_series.items():\n",
    "        if 1 <= grid_id <= 10000:\n",
    "            r = int((grid_id - 1) // 100)\n",
    "            c = int((grid_id - 1) % 100)\n",
    "            grid_matrix[r, c] = val\n",
    "\n",
    "    # --- PLOT 3D ---\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x = np.arange(0, 100, 1)\n",
    "    y = np.arange(0, 100, 1)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "\n",
    "    surf = ax.plot_surface(X, Y, grid_matrix, cmap=cmap, edgecolor='none', alpha=0.9)\n",
    "\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel('Grid X')\n",
    "    ax.set_ylabel('Grid Y')\n",
    "    ax.set_zlabel('Volume')\n",
    "\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=10, label='Volumen')\n",
    "    ax.view_init(elev=30, azim=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# 1. SMS Spatial Distribution\n",
    "plot_3d_surface(spatial_df['sms_total'], 'Distribuci\u00f3n Espacial 3D - SMS', '../imagenes/spatial_sms.png', cmap='inferno')\n",
    "\n",
    "# 2. Internet Spatial Distribution\n",
    "plot_3d_surface(spatial_df['internet'], 'Distribuci\u00f3n Espacial 3D - Internet', '../imagenes/spatial_internet.png', cmap='viridis')\n",
    "\n",
    "# 3. Calls Spatial Distribution\n",
    "plot_3d_surface(spatial_df['call_total'], 'Distribuci\u00f3n Espacial 3D - Llamadas', '../imagenes/spatial_calls.png', cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An\u00e1lisis Relacional: Picos, D\u00edas y Zonas\n",
    "\n",
    "NUEVO: Identificamos los d\u00edas de mayor tr\u00e1fico, establecemos un umbral y analizamos qu\u00e9 zonas contribuyeron m\u00e1s en esos picos hist\u00f3ricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. AN\u00c1LISIS DIARIO GLOBAL\n",
    "daily_agg = final_df.resample('D').sum()\n",
    "daily_agg['total_traffic'] = daily_agg[services].sum(axis=1)\n",
    "\n",
    "# C\u00e1lculo de Umbral (Mean + 2*Std)\n",
    "mean_val = daily_agg['total_traffic'].mean()\n",
    "std_val = daily_agg['total_traffic'].std()\n",
    "threshold = mean_val + 2 * std_val\n",
    "\n",
    "top_5_days = daily_agg.nlargest(5, 'total_traffic')\n",
    "\n",
    "print(f\"Umbral calculado: {threshold:,.2f}\")\n",
    "print(\"Top 5 D\u00edas con m\u00e1s tr\u00e1fico:\")\n",
    "print(top_5_days['total_traffic'])\n",
    "\n",
    "# Plot Diario\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_agg.index, daily_agg['total_traffic'], label='Tr\u00e1fico Total Diario', color='steelblue')\n",
    "plt.axhline(threshold, color='red', linestyle='--', linewidth=2, label='Umbral (Mean + 2Std)')\n",
    "plt.scatter(top_5_days.index, top_5_days['total_traffic'], color='red', s=100, zorder=5, label='Picos')\n",
    "plt.title('Evoluci\u00f3n Diaria del Tr\u00e1fico y Detecci\u00f3n de Picos', fontsize=16)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('../imagenes/daily_peaks_threshold.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DRILL-DOWN: Zonas Top en D\u00edas Top\n",
    "print(\"Desglosando zonas para los 5 d\u00edas pico...\")\n",
    "top_dates_str = top_5_days.index.strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "# Filtrar Dask DataFrame original\n",
    "ddf['DateStr'] = ddf['Timestamp'].dt.strftime('%Y-%m-%d')\n",
    "ddf_peaks = ddf[ddf['DateStr'].isin(top_dates_str)]\n",
    "ddf_peaks['total_load'] = ddf_peaks['internet'] + ddf_peaks['smsin'] + ddf_peaks['smsout'] + ddf_peaks['callin'] + ddf_peaks['callout']\n",
    "\n",
    "# Agrupar\n",
    "peak_zones_task = ddf_peaks.groupby(['DateStr', 'GridID'])['total_load'].sum()\n",
    "\n",
    "with ProgressBar():\n",
    "    peak_zones_df = peak_zones_task.compute().reset_index()\n",
    "\n",
    "# Preparar datos para Stacked Bar\n",
    "plot_data = []\n",
    "for date_str in top_dates_str:\n",
    "    day_data = peak_zones_df[peak_zones_df['DateStr'] == date_str]\n",
    "    if day_data.empty: continue\n",
    "    \n",
    "    # Top 5 zonas del d\u00eda\n",
    "    day_sorted = day_data.sort_values('total_load', ascending=False)\n",
    "    top_zones = day_sorted.head(5)\n",
    "    other_load = day_sorted.iloc[5:]['total_load'].sum()\n",
    "    \n",
    "    row = {'Date': date_str, 'Others': other_load}\n",
    "    for _, z in top_zones.iterrows():\n",
    "        row[f\"Zone {int(z['GridID'])}\"] = z['total_load']\n",
    "    plot_data.append(row)\n",
    "\n",
    "df_plot = pd.DataFrame(plot_data).set_index('Date')\n",
    "\n",
    "df_plot.plot(kind='bar', stacked=True, figsize=(12, 7), colormap='tab20')\n",
    "plt.title('Contribuci\u00f3n de Zonas en D\u00edas de Pico', fontsize=15)\n",
    "plt.ylabel('Tr\u00e1fico Total')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../imagenes/top_days_zones_breakdown.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. AN\u00c1LISIS HORARIO\n",
    "final_df['Hour'] = final_df.index.hour\n",
    "final_df['Total'] = final_df[services].sum(axis=1)\n",
    "hourly_avg = final_df.groupby('Hour')['Total'].mean()\n",
    "\n",
    "top_hours = hourly_avg.nlargest(5)\n",
    "print(\"Mayores horas de tr\u00e1fico promedio:\")\n",
    "print(top_hours)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "hourly_avg.plot(kind='bar', color='orange', alpha=0.7)\n",
    "plt.title('Perfil Horario Promedio (0-24h)', fontsize=14)\n",
    "plt.ylabel('Tr\u00e1fico Promedio')\n",
    "plt.xlabel('Hora')\n",
    "plt.savefig('../imagenes/hourly_profile.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evoluci\u00f3n Temporal del Tr\u00e1fico SMS (Animaci\u00f3n)\n",
    "\n",
    "Esta secci\u00f3n genera una animaci\u00f3n que muestra c\u00f3mo \"respira\" la ciudad. Visualizamos la evoluci\u00f3n del tr\u00e1fico SMS hora a hora durante la semana seleccionada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PREPARACI\u00d3N DE DATOS PARA ANIMACI\u00d3N ---\n",
    "print(\"Preparando datos para la animaci\u00f3n (esto puede tardar)... \")\n",
    "\n",
    "# Filtrar datos para la semana seleccionada usando Dask\n",
    "# Convertir TimeInterval a datetime para filtrar\n",
    "ddf['Timestamp'] = dd.to_datetime(ddf['TimeInterval'], unit='ms')\n",
    "ddf_week = ddf[(ddf['Timestamp'] >= start_date) & (ddf['Timestamp'] < end_date)]\n",
    "\n",
    "# Agrupar por Hora y GridID para reducir el tama\u00f1o de los datos\n",
    "# Redondeamos el timestamp a la hora m\u00e1s cercana\n",
    "ddf_week['Hour'] = ddf_week['Timestamp'].dt.floor('h')\n",
    "\n",
    "# Sumar SMS (in + out)\n",
    "ddf_week['sms_total'] = ddf_week['smsin'] + ddf_week['smsout']\n",
    "\n",
    "# Agrupar\n",
    "anim_task = ddf_week.groupby(['Hour', 'GridID'])['sms_total'].sum()\n",
    "\n",
    "with ProgressBar():\n",
    "    anim_df = anim_task.compute().reset_index()\n",
    "\n",
    "print(\"Datos procesados. Generando animaci\u00f3n...\")\n",
    "\n",
    "# Obtener lista de horas \u00fanicas ordenadas\n",
    "hours = sorted(anim_df['Hour'].unique())\n",
    "\n",
    "# Configurar figura 3D\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlabel('Grid X')\n",
    "ax.set_ylabel('Grid Y')\n",
    "ax.set_zlabel('SMS Volume')\n",
    "ax.set_zlim(0, anim_df['sms_total'].max() * 0.8) # Fijar eje Z para estabilidad visual\n",
    "\n",
    "x = np.arange(0, 100, 1)\n",
    "y = np.arange(0, 100, 1)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Funci\u00f3n de actualizaci\u00f3n para la animaci\u00f3n\n",
    "def update_plot(frame_idx):\n",
    "    ax.clear()\n",
    "    current_hour = hours[frame_idx]\n",
    "    \n",
    "    # Filtrar datos para la hora actual\n",
    "    hour_data = anim_df[anim_df['Hour'] == current_hour]\n",
    "    \n",
    "    # Construir matriz\n",
    "    grid_matrix = np.zeros((100, 100))\n",
    "    for _, row in hour_data.iterrows():\n",
    "        grid_id = int(row['GridID'])\n",
    "        if 1 <= grid_id <= 10000:\n",
    "            r = (grid_id - 1) // 100\n",
    "            c = (grid_id - 1) % 100\n",
    "            grid_matrix[r, c] = row['sms_total']\n",
    "            \n",
    "    # Plotear superficie\n",
    "    surf = ax.plot_surface(X, Y, grid_matrix, cmap='inferno', edgecolor='none')\n",
    "    ax.set_title(f'Tr\u00e1fico SMS - {current_hour.strftime(\"%A %H:%M\")}', fontsize=16)\n",
    "    ax.set_zlim(0, anim_df['sms_total'].max() * 0.8)\n",
    "    ax.view_init(elev=30, azim=45)\n",
    "    return surf,\n",
    "\n",
    "# Crear animaci\u00f3n\n",
    "ani = animation.FuncAnimation(fig, update_plot, frames=len(hours), interval=200, blit=False)\n",
    "\n",
    "# Mostrar animaci\u00f3n en el notebook\n",
    "plt.close() # Cerrar plot est\u00e1tico inicial\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "report_content = f\"\"\"# Reporte de An\u00e1lisis de Tr\u00e1fico Avanzado\\n",
    "Fecha de generaci\u00f3n: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n",
    "\\n",
    "## 1. Picos y Umbrales\\n",
    "![Picos Diarios](imagenes/daily_peaks_threshold.png)\\n",
    "\\n",
    "## 2. Zonas en D\u00edas Pico\\n",
    "![Zonas Top](imagenes/top_days_zones_breakdown.png)\\n",
    "\\n",
    "## 3. Perfil Horario\\n",
    "![Horas Top](imagenes/hourly_profile.png)\\n",
    "\\n",
    "## 4. An\u00e1lisis Semanal (Aleatorio)\\n",
    "![Semana](imagenes/internet_sms_call_traffic.png)\\n",
    "\\n",
    "## 5. Distribuci\u00f3n Espacial General\\n",
    "![Mapa 3D](imagenes/spatial_internet.png)\\n",
    "\"\"\"\n",
    "\n",
    "with open('../analysis_report_advanced.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(\"Reporte actualizado generado: ../analysis_report_advanced.md\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}