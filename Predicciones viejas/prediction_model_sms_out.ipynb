{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask version: 2025.11.0\n",
      "Dask dataframe imported successfully.\n",
      "Torch version: 2.9.1+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try:\n",
    "    import dask\n",
    "    import dask.dataframe as dd\n",
    "    print(f\"Dask version: {dask.__version__}\")\n",
    "    print(\"Dask dataframe imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing dask: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"Torch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing torch: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentaci\u00f3n del Modelo de Predicci\u00f3n de Tr\u00e1fico (ST-DenseNet)\n",
    "\n",
    "Este documento detalla la arquitectura, formulaci\u00f3n matem\u00e1tica y metodolog\u00eda utilizada en el modelo de predicci\u00f3n de tr\u00e1fico celular (SMS) para la ciudad de Mil\u00e1n. El modelo se basa en una arquitectura **ST-DenseNet** (Spatio-Temporal Densely Connected Convolutional Network).\n",
    "\n",
    "## 1. Definici\u00f3n del Problema\n",
    "\n",
    "El objetivo es predecir el volumen de tr\u00e1fico de SMS (entrante y saliente) para cada celda de una cuadr\u00edcula de $100 \\times 100$ en la ciudad de Mil\u00e1n para el siguiente intervalo de tiempo, bas\u00e1ndose en datos hist\u00f3ricos.\n",
    "\n",
    "Sea $X_t \\in \\mathbb{R}^{2 \\times 100 \\times 100}$ el tensor de tr\u00e1fico en el tiempo $t$, donde el canal 0 representa `smsin` y el canal 1 representa `smsout`. El objetivo es aprender una funci\u00f3n $f$ tal que:\n",
    "\n",
    "$$ \\hat{X}_{t} = f(X_{t-1}, X_{t-2}, \\dots, X_{t-n}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "GPU: NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Configuraci\u00f3n del Dispositivo\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Par\u00e1metros del Modelo\n",
    "H, W = 100, 100  # Dimensiones de la cuadr\u00edcula\n",
    "LEN_CLOSE = 3    # Dependencia de proximidad (p)\n",
    "LEN_PERIOD = 3   # Dependencia de periodo (q)\n",
    "NB_FLOW = 1      # Canales (SMS In, SMS Out)\n",
    "\n",
    "# Par\u00e1metros de Entrenamiento\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de Datos\n",
    "\n",
    "### 2.1. Agregaci\u00f3n Espacio-Temporal\n",
    "Los datos crudos se agregan espacialmente en una rejilla de $H \\times W$ ($100 \\times 100$) y temporalmente en intervalos de 1 hora.\n",
    "\n",
    "### 2.2. Normalizaci\u00f3n Min-Max\n",
    "Para facilitar la convergencia del entrenamiento, los datos se normalizan al rango $[0, 1]$ utilizando la transformaci\u00f3n Min-Max:\n",
    "\n",
    "$$ x'_{i} = \\frac{x_i - \\min(X)}{\\max(X) - \\min(X)} $$\n",
    "\n",
    "Donde $x_i$ es el valor de tr\u00e1fico en una celda y tiempo espec\u00edfico, y $X$ es el conjunto total de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando y agregando datos (esto puede tardar)...\n",
      "[########################################] | 100% Completed | 68.61 s\n",
      "Agregaci\u00f3n completada.\n",
      "                 Hour  GridID     smsin    smsout\n",
      "0 2013-12-30 23:00:00   331.0  0.685714  0.937103\n",
      "1 2013-12-30 23:00:00   378.0  1.229755  0.444860\n",
      "2 2013-12-30 23:00:00   408.0  1.235510  0.924064\n",
      "3 2013-12-30 23:00:00   427.0  0.685714  0.937103\n",
      "4 2013-12-30 23:00:00  3248.0  1.800727  0.936475\n"
     ]
    }
   ],
   "source": [
    "files = ['../data1.csv/data1.csv', '../data2.csv/data2.csv']\n",
    "valid_files = [f for f in files if os.path.exists(f)]\n",
    "\n",
    "print(\"Cargando y agregando datos (esto puede tardar)...\")\n",
    "\n",
    "# Cargar con Dask\n",
    "ddf = dd.read_csv(valid_files, assume_missing=True)\n",
    "\n",
    "# Convertir TimeInterval a datetime\n",
    "ddf['Timestamp'] = dd.to_datetime(ddf['TimeInterval'], unit='ms')\n",
    "\n",
    "# Redondear a la hora (Agregaci\u00f3n Temporal)\n",
    "ddf['Hour'] = ddf['Timestamp'].dt.floor('h')\n",
    "\n",
    "# Seleccionar columnas de inter\u00e9s (SMS)\n",
    "cols = ['Hour', 'GridID', 'smsout']\n",
    "ddf = ddf[cols]\n",
    "\n",
    "# Agrupar por Hora y GridID\n",
    "agg_task = ddf.groupby(['Hour', 'GridID'])[['smsout']].sum()\n",
    "\n",
    "with ProgressBar():\n",
    "    df_agg = agg_task.compute().reset_index()\n",
    "\n",
    "print(\"Agregaci\u00f3n completada.\")\n",
    "print(df_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango temporal: 2013-10-31 23:00:00 a 2014-01-01 22:00:00 (1488 horas)\n",
      "Rellenando tensor 4D...\n",
      "Tensor shape: (1488, 2, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# --- CONSTRUCCI\u00d3N DE LA MATRIZ 4D (Time, Channels, H, W) ---\n",
    "\n",
    "# Filtrar GridIDs v\u00e1lidos (1 a 10000)\n",
    "df_agg = df_agg[(df_agg['GridID'] >= 1) & (df_agg['GridID'] <= 10000)]\n",
    "\n",
    "# Crear \u00edndice temporal completo\n",
    "min_time = df_agg['Hour'].min()\n",
    "max_time = df_agg['Hour'].max()\n",
    "time_range = pd.date_range(min_time, max_time, freq='H')\n",
    "\n",
    "print(f\"Rango temporal: {min_time} a {max_time} ({len(time_range)} horas)\")\n",
    "\n",
    "# Inicializar tensor gigante: [Time, 2, 100, 100]\n",
    "    data_tensor = np.zeros((len(time_range), 1, 100, 100), dtype=np.float32)\n",
    "\n",
    "# Mapeo de tiempo a \u00edndice\n",
    "time_to_idx = {t: i for i, t in enumerate(time_range)}\n",
    "\n",
    "print(\"Rellenando tensor 4D...\")\n",
    "# Iterar y rellenar (esto puede ser lento en Python puro, pero pandas lo facilita)\n",
    "# Pivotar tabla para tener GridID como columnas\n",
    "    df_pivot = df_agg.pivot_table(index='Hour', columns='GridID', values=['smsout'], fill_value=0)\n",
    "\n",
    "# Rellenar el tensor\n",
    "for t in time_range:\n",
    "    if t in df_pivot.index:\n",
    "        idx = time_to_idx[t]\n",
    "        # smsout\n",
    "        vals = df_pivot.loc[t, 'smsout'].reindex(range(1, 10001), fill_value=0).values\n",
    "        data_tensor[idx, 0, :, :] = vals.reshape(100, 100)\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min original: 0.0, Max original: 6283.77294921875\n",
      "Min norm: 0.0, Max norm: 1.0\n"
     ]
    }
   ],
   "source": [
    "# --- NORMALIZACI\u00d3N MIN-MAX [0, 1] ---\n",
    "mmn_min = data_tensor.min()\n",
    "mmn_max = data_tensor.max()\n",
    "\n",
    "print(f\"Min original: {mmn_min}, Max original: {mmn_max}\")\n",
    "\n",
    "data_norm = (data_tensor - mmn_min) / (mmn_max - mmn_min)\n",
    "\n",
    "print(f\"Min norm: {data_norm.min()}, Max norm: {data_norm.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Construcci\u00f3n de Entradas (Dependencias Temporales)\n",
    "El modelo captura dos tipos de dependencias temporales:\n",
    "\n",
    "1.  **Cercan\u00eda (Closeness - $X_c$)**: Captura la tendencia reciente. Se toman los \u00faltimos $l_c$ intervalos de tiempo.\n",
    "    $$ X_c = [X_{t-l_c}, X_{t-(l_c-1)}, \\dots, X_{t-1}] $$\n",
    "    \n",
    "2.  **Periodo (Period - $X_d$)**: Captura la periodicidad diaria (mismo hora del d\u00eda en d\u00edas anteriores). Se toman $l_p$ d\u00edas.\n",
    "    $$ X_d = [X_{t-l_p \\cdot 24}, X_{t-(l_p-1) \\cdot 24}, \\dots, X_{t-24}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando tensores Xc, Xd, Y...\n",
      "Generando dataset (Ventana Deslizante)...\n",
      "XC shape: (1416, 6, 100, 100)\n",
      "XD shape: (1416, 6, 100, 100)\n",
      "Y shape:  (1416, 2, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# --- GENERACI\u00d3N DE DATASET (Ventana Deslizante) ---\n",
    "\n",
    "def create_dataset(data, len_c, len_p):\n",
    "    # data: [T, C, H, W]\n",
    "    T, C, H, W = data.shape\n",
    "    \n",
    "    XC, XD, Y = [], [], []\n",
    "    \n",
    "    # Empezamos despu\u00e9s de tener suficiente historia para Periodo (que es el m\u00e1s largo)\n",
    "    # Periodo necesita: t - len_p * 24\n",
    "    start_idx = len_p * 24\n",
    "    \n",
    "    print(\"Generando dataset (Ventana Deslizante)...\")\n",
    "    for i in range(start_idx, T):\n",
    "        # Target\n",
    "        y = data[i] # [C, H, W]\n",
    "        \n",
    "        # Proximidad (Close): \u00faltimos len_c horas\n",
    "        # [i-len_c, ..., i-1]\n",
    "        xc = data[i - len_c : i] # [len_c, C, H, W]\n",
    "        # Concatenar en canales: [len_c * C, H, W]\n",
    "        xc = np.concatenate(xc, axis=0)\n",
    "        \n",
    "        # Periodo (Period): misma hora d\u00edas anteriores\n",
    "        # [i - len_p*24, ..., i - 24]\n",
    "        xd = []\n",
    "        for p in range(len_p, 0, -1):\n",
    "            idx = i - p * 24\n",
    "            xd.append(data[idx])\n",
    "        xd = np.concatenate(xd, axis=0) # [len_p * C, H, W]\n",
    "        \n",
    "        XC.append(xc)\n",
    "        XD.append(xd)\n",
    "        Y.append(y)\n",
    "        \n",
    "    return np.array(XC), np.array(XD), np.array(Y)\n",
    "\n",
    "print(\"Generando tensores Xc, Xd, Y...\")\n",
    "XC, XD, Y = create_dataset(data_norm, LEN_CLOSE, LEN_PERIOD)\n",
    "print(f\"XC shape: {XC.shape}\")\n",
    "print(f\"XD shape: {XD.shape}\")\n",
    "print(f\"Y shape:  {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1248\n",
      "Test samples:  168\n"
     ]
    }
   ],
   "source": [
    "# --- DIVISI\u00d3N TRAIN / TEST ---\n",
    "# Test = \u00daltimos 7 d\u00edas (7 * 24 horas = 168 horas)\n",
    "test_hours = 7 * 24\n",
    "total_samples = len(Y)\n",
    "train_samples = total_samples - test_hours\n",
    "\n",
    "XC_train, XD_train, Y_train = XC[:train_samples], XD[:train_samples], Y[:train_samples]\n",
    "XC_test, XD_test, Y_test = XC[train_samples:], XD[train_samples:], Y[train_samples:]\n",
    "\n",
    "print(f\"Train samples: {len(Y_train)}\")\n",
    "print(f\"Test samples:  {len(Y_test)}\")\n",
    "\n",
    "# Convertir a Tensores PyTorch\n",
    "train_data = TensorDataset(torch.from_numpy(XC_train).float(), torch.from_numpy(XD_train).float(), torch.from_numpy(Y_train).float())\n",
    "test_data = TensorDataset(torch.from_numpy(XC_test).float(), torch.from_numpy(XD_test).float(), torch.from_numpy(Y_test).float())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El enfoque principal del estudio es proponer una arquitectura de **Red Neuronal Convolucional Densely Connected (Densely Connected CNN)** para modelar colectivamente las complejas dependencias espaciales y temporales del tr\u00e1fico celular.\n",
    "\n",
    "Esta arquitectura, considerada como el modelo de aprendizaje profundo m\u00e1s avanzado en este contexto, est\u00e1 dise\u00f1ada espec\u00edficamente para superar las limitaciones de los modelos estad\u00edsticos lineales y de los enfoques de aprendizaje autom\u00e1tico que se centran \u00fanicamente en celdas individuales.\n",
    "\n",
    "A continuaci\u00f3n, se detalla la arquitectura del modelo de predicci\u00f3n, siguiendo la descripci\u00f3n provista en las fuentes:\n",
    "\n",
    "---\n",
    "\n",
    "## Arquitectura del Modelo: Redes Neuronales Convolucionales Densely Connected\n",
    "\n",
    "El marco de predicci\u00f3n se ilustra conceptualmente en la Figura 2 (mencionada en las fuentes) y consta principalmente de tres componentes: construcci\u00f3n de datos de entrenamiento (que genera los tensores de entrada $X_c$ y $X_d$), el aprendizaje convolucional y la fusi\u00f3n basada en matriz param\u00e9trica.\n",
    "\n",
    "### I. Dise\u00f1o de Doble Red con Estructura Compartida\n",
    "\n",
    "La arquitectura se compone de **dos redes** con una **estructura compartida**. Esta dualidad es fundamental para modelar la dependencia temporal del tr\u00e1fico:\n",
    "\n",
    "1.  **Red de Proximidad (Closeness):** Utiliza el tensor de entrada $X_c$ para modelar la **dependencia temporal de corto alcance** (tr\u00e1fico reciente).\n",
    "2.  **Red de Periodo (Period):** Utiliza el tensor de entrada $X_d$ para modelar la **dependencia temporal de largo alcance** (historia diaria o patrones peri\u00f3dicos).\n",
    "\n",
    "### II. Componentes de las Capas y Conexiones Densas\n",
    "\n",
    "Dentro de cada una de las dos redes, la arquitectura implementa el patr\u00f3n **Densely Connected**:\n",
    "\n",
    "#### A. Operaci\u00f3n de Conexi\u00f3n Densa ($\\oplus$)\n",
    "\n",
    "El patr\u00f3n *Densely Connected* se adopta para mitigar el problema del gradiente evanescente y para fortalecer la propagaci\u00f3n de caracter\u00edsticas.\n",
    "\n",
    "*   En la $l$-\u00e9sima capa, la salida ($X^l$) es la transformaci\u00f3n no lineal ($f_l$) de la concatenaci\u00f3n ($\\oplus$) de los mapas de caracter\u00edsticas producidos en **todas las capas precedentes**.\n",
    "*   Para la dependencia de proximidad, esto se expresa matem\u00e1ticamente como:\n",
    "    $$X^l_c = f_l(X^0_c \\oplus X^1_c \\oplus \\cdots \\oplus X^{l-1}_c) \\quad \\text{}$$\n",
    "\n",
    "#### B. Componentes de Capa ($f_l(\\cdot)$)\n",
    "\n",
    "Cada capa $l$ implementa una **transformaci\u00f3n no lineal** $f_l(\\cdot)$ que es una funci\u00f3n compuesta por tres operaciones consecutivas:\n",
    "\n",
    "1.  **Convoluci\u00f3n (Conv):** Utiliza filtros para fusionar la informaci\u00f3n de celdas vecinas y capturar la dependencia espacial de manera jer\u00e1rquica.\n",
    "    *   **Configuraci\u00f3n General:** Casi todas las capas de convoluci\u00f3n tienen **32 filtros con un tama\u00f1o de $3 \\times 3$**.\n",
    "    *   **Capa Final:** La \u00faltima capa (previa a la fusi\u00f3n) tiene **2 filtros con un tama\u00f1o de $3 \\times 3$**.\n",
    "2.  **Normalizaci\u00f3n por Lotes (BN):** Ayuda a estabilizar el proceso de aprendizaje.\n",
    "3.  **Unidades Lineales Rectificadas (ReLU):** Es la funci\u00f3n de activaci\u00f3n no lineal que decide si una neurona debe activarse o no, calculando la suma ponderada de su entrada.\n",
    "\n",
    "### III. Fusi\u00f3n Basada en Matriz Param\u00e9trica (Parametric Matrix Based Fusion)\n",
    "\n",
    "La fase final de la arquitectura consiste en un esquema de fusi\u00f3n dise\u00f1ado para combinar las caracter\u00edsticas extra\u00eddas de las dos redes, reconociendo que la relevancia de la proximidad y el periodo **var\u00eda**.\n",
    "\n",
    "#### A. Capa Pre-Fusi\u00f3n\n",
    "Primero, se a\u00f1ade una capa de convoluci\u00f3n separada en la parte superior de la $L$-\u00e9sima capa de ambas redes para obtener las caracter\u00edsticas que se utilizar\u00e1n en el esquema de fusi\u00f3n param\u00e9trica. Las salidas de estas capas son denotadas como $X^{L+1}_c$ y $X^{L+1}_d$.\n",
    "\n",
    "#### B. F\u00f3rmula de Fusi\u00f3n\n",
    "La salida intermedia fusionada ($X_o$) se calcula mediante la siguiente f\u00f3rmula:\n",
    "$$X_o = W_c \\odot X^{L+1}_c + W_d \\odot X^{L+1}_d \\quad \\text{}$$\n",
    "\n",
    "*   **Producto de Hadamard ($\\odot$):** Es la multiplicaci\u00f3n elemento a elemento.\n",
    "*   **$W_c$ y $W_d$:** Son **par\u00e1metros aprendibles** (*learnable parameters*) que caracterizan el grado de influencia de la dependencia de proximidad y periodo, respectivamente, en la predicci\u00f3n del tr\u00e1fico inal\u00e1mbrico.\n",
    "\n",
    "#### C. Activaci\u00f3n Final\n",
    "La predicci\u00f3n final del tr\u00e1fico ($\\hat{X}_t$) se obtiene aplicando la funci\u00f3n **Sigmoide ($\\sigma$)** a la salida fusionada:\n",
    "$$\\hat{X}_t = \\sigma(X_o) \\quad \\text{}$$\n",
    "(Se utiliza la Sigmoide debido a que el tr\u00e1fico de entrada fue escalado previamente al rango $$ mediante la normalizaci\u00f3n Min-Max).\n",
    "\n",
    "En resumen, la arquitectura Densely Connected CNN utiliza una estructura de doble rama para procesar las dos dimensiones temporales, emplea la convoluci\u00f3n para modelar las correlaciones espaciales dentro de cada rama, y utiliza un mecanismo de fusi\u00f3n param\u00e9trico para aprender la contribuci\u00f3n relativa de cada dependencia temporal en la predicci\u00f3n final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=32):\n",
    "        super(DenseUnit, self).__init__()\n",
    "        # Text specifies: Conv -> BN -> ReLU\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        # Concatenar entrada y salida (Conexi\u00f3n Densa)\n",
    "        return torch.cat([x, out], 1)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers=3, growth_rate=32):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_channels = in_channels\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(DenseUnit(current_channels, growth_rate))\n",
    "            current_channels += growth_rate\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class STDenseNet(nn.Module):\n",
    "    def __init__(self, len_c, len_p, nb_flow, growth_rate=32):\n",
    "        super(STDenseNet, self).__init__()\n",
    "        \n",
    "        # Input channels for each branch\n",
    "        c_in = len_c * nb_flow\n",
    "        p_in = len_p * nb_flow\n",
    "        \n",
    "        # --- Branch Proximity (Close) ---\n",
    "        # Entrada -> Conv -> DenseBlock -> Conv\n",
    "        self.c_conv1 = nn.Conv2d(c_in, growth_rate, kernel_size=3, padding=1)\n",
    "        self.c_dense = DenseBlock(growth_rate, num_layers=3, growth_rate=growth_rate)\n",
    "        # Output channels after dense block: growth_rate + 3 * growth_rate = 4 * 32 = 128\n",
    "        dense_out_c = growth_rate + 3 * growth_rate\n",
    "        self.c_conv2 = nn.Conv2d(dense_out_c, nb_flow, kernel_size=3, padding=1)\n",
    "        \n",
    "        # --- Branch Period ---\n",
    "        self.p_conv1 = nn.Conv2d(p_in, growth_rate, kernel_size=3, padding=1)\n",
    "        self.p_dense = DenseBlock(growth_rate, num_layers=3, growth_rate=growth_rate)\n",
    "        self.p_conv2 = nn.Conv2d(dense_out_c, nb_flow, kernel_size=3, padding=1)\n",
    "        \n",
    "        # --- Fusion ---\n",
    "        # Parametric Matrix Fusion: Wc * Xc + Wd * Xd\n",
    "        # Wc, Wd are learnable parameters of shape (nb_flow, H, W)\n",
    "        self.Wc = nn.Parameter(torch.randn(nb_flow, H, W), requires_grad=True)\n",
    "        self.Wd = nn.Parameter(torch.randn(nb_flow, H, W), requires_grad=True)\n",
    "        \n",
    "        # Output Activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xc, xd):\n",
    "        # Proximity Branch\n",
    "        out_c = self.c_conv1(xc)\n",
    "        out_c = self.c_dense(out_c)\n",
    "        out_c = self.c_conv2(out_c)\n",
    "        \n",
    "        # Period Branch\n",
    "        out_p = self.p_conv1(xd)\n",
    "        out_p = self.p_dense(out_p)\n",
    "        out_p = self.p_conv2(out_p)\n",
    "        \n",
    "        # Fusion\n",
    "        fusion = out_c * self.Wc + out_p * self.Wd\n",
    "        \n",
    "        # Output\n",
    "        return self.sigmoid(fusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento\n",
    "\n",
    "### 4.1. Funci\u00f3n de P\u00e9rdida (Loss Function)\n",
    "Se utiliza el Error Cuadr\u00e1tico Medio (MSE) entre la predicci\u00f3n y el valor real:\n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} || \\hat{X}_t^{(i)} - X_t^{(i)} ||^2 $$\n",
    "\n",
    "Donde $\\theta$ son todos los par\u00e1metros aprendibles del modelo.\n",
    "\n",
    "### 4.2. Optimizador\n",
    "*   **Algoritmo**: Adam (Adaptive Moment Estimation).\n",
    "*   **Learning Rate**: Se utiliza un esquema de decaimiento (MultiStepLR) que reduce la tasa de aprendizaje en \u00e9pocas espec\u00edficas (50 y 75) para refinar la convergencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en: cuda\n",
      "GPU Activa: NVIDIA GeForce RTX 5080\n",
      "Nota: Si el uso de GPU no sube inmediatamente, es porque se est\u00e1n cargando datos en RAM.\n",
      "Epoch [10/100], Loss: 0.007830\n",
      "Epoch [20/100], Loss: 0.007011\n",
      "Epoch [30/100], Loss: 0.006453\n",
      "Epoch [40/100], Loss: 0.006150\n",
      "Epoch [50/100], Loss: 0.005888\n",
      "Epoch [60/100], Loss: 0.005799\n",
      "Epoch [70/100], Loss: 0.005738\n",
      "Epoch [80/100], Loss: 0.005654\n"
     ]
    }
   ],
   "source": [
    "model = STDenseNet(LEN_CLOSE, LEN_PERIOD, NB_FLOW).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "\n",
    "print(f\"Iniciando entrenamiento en: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Activa: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"Nota: Si el uso de GPU no sube inmediatamente, es porque se est\u00e1n cargando datos en RAM.\")\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for xc, xd, y in train_loader:\n",
    "        xc, xd, y = xc.to(DEVICE), xd.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(xc, xd)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.ylim(0, 0.015)\n",
    "plt.title('Curva de Aprendizaje')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretaci\u00f3n de Resultados\n",
    "\n",
    "### 5.1. M\u00e9tricas de Evaluaci\u00f3n\n",
    "Para evaluar el rendimiento, primero se **desnormalizan** las predicciones para volver a la escala original de tr\u00e1fico (n\u00famero de SMS).\n",
    "\n",
    "$$ \\hat{X}_{orig} = \\hat{X}_{norm} \\cdot (Max - Min) + Min $$\n",
    "\n",
    "Se calculan las siguientes m\u00e9tricas:\n",
    "\n",
    "1.  **RMSE (Root Mean Squared Error)**: Error cuadr\u00e1tico medio. Penaliza m\u00e1s los errores grandes.\n",
    "    $$ RMSE = \\sqrt{\\frac{1}{M} \\sum (\\hat{x}_{orig} - x_{orig})^2} $$\n",
    "\n",
    "2.  **MAE (Mean Absolute Error)**: Error absoluto medio. Es m\u00e1s robusto a outliers y f\u00e1cil de interpretar (error promedio en SMS).\n",
    "    $$ MAE = \\frac{1}{M} \\sum |\\hat{x}_{orig} - x_{orig}| $$\n",
    "\n",
    "3.  **R2 Score**: Coeficiente de determinaci\u00f3n. Indica qu\u00e9 tan bien las predicciones se ajustan a los datos reales (1.0 es perfecto).\n",
    "\n",
    "### 5.2. Visualizaci\u00f3n\n",
    "*   **Scatter Plot**: Gr\u00e1fico de dispersi\u00f3n de Predicci\u00f3n vs Realidad. Idealmente, los puntos deber\u00edan alinearse en la diagonal.\n",
    "*   **Distribuci\u00f3n de Errores**: Histograma de los residuos. Deber\u00eda estar centrado en 0.\n",
    "*   **Mapa de Calor de Error**: Muestra d\u00f3nde se concentran los errores espacialmente.\n",
    "*   **Serie Temporal**: Comparaci\u00f3n visual de la predicci\u00f3n y la realidad a lo largo del tiempo para una celda espec\u00edfica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}