{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dask version: 2025.11.0\n",
      "Dask dataframe imported successfully.\n",
      "Torch version: 2.9.1+cu128\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "try:\n",
    "    import dask\n",
    "    import dask.dataframe as dd\n",
    "    print(f\"Dask version: {dask.__version__}\")\n",
    "    print(\"Dask dataframe imported successfully.\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing dask: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"Torch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing torch: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentación del Modelo de Predicción de Tráfico (ST-DenseNet)\n",
    "\n",
    "Este documento detalla la arquitectura, formulación matemática y metodología utilizada en el modelo de predicción de tráfico celular (SMS) para la ciudad de Milán. El modelo se basa en una arquitectura **ST-DenseNet** (Spatio-Temporal Densely Connected Convolutional Network).\n",
    "\n",
    "## 1. Definición del Problema\n",
    "\n",
    "El objetivo es predecir el volumen de tráfico de SMS (entrante y saliente) para cada celda de una cuadrícula de $100 \\times 100$ en la ciudad de Milán para el siguiente intervalo de tiempo, basándose en datos históricos.\n",
    "\n",
    "Sea $X_t \\in \\mathbb{R}^{2 \\times 100 \\times 100}$ el tensor de tráfico en el tiempo $t$, donde el canal 0 representa `smsin` y el canal 1 representa `smsout`. El objetivo es aprender una función $f$ tal que:\n",
    "\n",
    "$$ \\hat{X}_{t} = f(X_{t-1}, X_{t-2}, \\dots, X_{t-n}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "GPU: NVIDIA GeForce RTX 5080\n"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Configuración del Dispositivo\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Parámetros del Modelo\n",
    "H, W = 100, 100  # Dimensiones de la cuadrícula\n",
    "LEN_CLOSE = 3    # Dependencia de proximidad (p)\n",
    "LEN_PERIOD = 3   # Dependencia de periodo (q)\n",
    "NB_FLOW = 2      # Canales (SMS In, SMS Out)\n",
    "\n",
    "# Parámetros de Entrenamiento\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de Datos\n",
    "\n",
    "### 2.1. Agregación Espacio-Temporal\n",
    "Los datos crudos se agregan espacialmente en una rejilla de $H \\times W$ ($100 \\times 100$) y temporalmente en intervalos de 1 hora.\n",
    "\n",
    "### 2.2. Normalización Min-Max\n",
    "Para facilitar la convergencia del entrenamiento, los datos se normalizan al rango $[0, 1]$ utilizando la transformación Min-Max:\n",
    "\n",
    "$$ x'_{i} = \\frac{x_i - \\min(X)}{\\max(X) - \\min(X)} $$\n",
    "\n",
    "Donde $x_i$ es el valor de tráfico en una celda y tiempo específico, y $X$ es el conjunto total de datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando y agregando datos (esto puede tardar)...\n",
      "[########################################] | 100% Completed | 71.40 s\n",
      "Agregación completada.\n",
      "                 Hour  GridID      smsin     smsout\n",
      "0 2013-12-28 23:00:00   592.0   0.027016   0.043677\n",
      "1 2013-12-28 23:00:00   649.0   3.977609   3.422556\n",
      "2 2013-12-28 23:00:00   659.0   0.989785  41.230774\n",
      "3 2013-12-28 23:00:00   709.0   0.569101   0.349390\n",
      "4 2013-12-28 23:00:00  5480.0  10.923906   5.051453\n"
     ]
    }
   ],
   "source": [
    "files = ['data1.csv/data1.csv', 'data2.csv/data2.csv']\n",
    "valid_files = [f for f in files if os.path.exists(f)]\n",
    "\n",
    "print(\"Cargando y agregando datos (esto puede tardar)...\")\n",
    "\n",
    "# Cargar con Dask\n",
    "ddf = dd.read_csv(valid_files, assume_missing=True)\n",
    "\n",
    "# Convertir TimeInterval a datetime\n",
    "ddf['Timestamp'] = dd.to_datetime(ddf['TimeInterval'], unit='ms')\n",
    "\n",
    "# Redondear a la hora (Agregación Temporal)\n",
    "ddf['Hour'] = ddf['Timestamp'].dt.floor('h')\n",
    "\n",
    "# Seleccionar columnas de interés (SMS)\n",
    "cols = ['Hour', 'GridID', 'smsin', 'smsout']\n",
    "ddf = ddf[cols]\n",
    "\n",
    "# Agrupar por Hora y GridID\n",
    "agg_task = ddf.groupby(['Hour', 'GridID'])[['smsin', 'smsout']].sum()\n",
    "\n",
    "with ProgressBar():\n",
    "    df_agg = agg_task.compute().reset_index()\n",
    "\n",
    "print(\"Agregación completada.\")\n",
    "print(df_agg.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rango temporal: 2013-10-31 23:00:00 a 2014-01-01 22:00:00 (1488 horas)\n",
      "Rellenando tensor 4D...\n",
      "Tensor shape: (1488, 2, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# --- CONSTRUCCIÓN DE LA MATRIZ 4D (Time, Channels, H, W) ---\n",
    "\n",
    "# Filtrar GridIDs válidos (1 a 10000)\n",
    "df_agg = df_agg[(df_agg['GridID'] >= 1) & (df_agg['GridID'] <= 10000)]\n",
    "\n",
    "# Crear índice temporal completo\n",
    "min_time = df_agg['Hour'].min()\n",
    "max_time = df_agg['Hour'].max()\n",
    "time_range = pd.date_range(min_time, max_time, freq='H')\n",
    "\n",
    "print(f\"Rango temporal: {min_time} a {max_time} ({len(time_range)} horas)\")\n",
    "\n",
    "# Inicializar tensor gigante: [Time, 2, 100, 100]\n",
    "data_tensor = np.zeros((len(time_range), 2, 100, 100), dtype=np.float32)\n",
    "\n",
    "# Mapeo de tiempo a índice\n",
    "time_to_idx = {t: i for i, t in enumerate(time_range)}\n",
    "\n",
    "print(\"Rellenando tensor 4D...\")\n",
    "# Iterar y rellenar (esto puede ser lento en Python puro, pero pandas lo facilita)\n",
    "# Pivotar tabla para tener GridID como columnas\n",
    "df_pivot = df_agg.pivot_table(index='Hour', columns='GridID', values=['smsin', 'smsout'], fill_value=0)\n",
    "\n",
    "# Rellenar el tensor\n",
    "for t in time_range:\n",
    "    if t in df_pivot.index:\n",
    "        idx = time_to_idx[t]\n",
    "        # smsin\n",
    "        smsin_grid = np.zeros((100, 100))\n",
    "        # Obtener valores para este tiempo. \n",
    "        # Nota: df_pivot['smsin'] tiene columnas 1..10000. \n",
    "        # Necesitamos mapear 1..10000 a 0..99, 0..99\n",
    "        \n",
    "        # Extraer array 1D de 10000 elementos (asegurando que existan todas las columnas)\n",
    "        # Para simplificar y asegurar orden, reindexamos\n",
    "        vals_in = df_pivot.loc[t, 'smsin'].reindex(range(1, 10001), fill_value=0).values\n",
    "        vals_out = df_pivot.loc[t, 'smsout'].reindex(range(1, 10001), fill_value=0).values\n",
    "        \n",
    "        data_tensor[idx, 0, :, :] = vals_in.reshape(100, 100)\n",
    "        data_tensor[idx, 1, :, :] = vals_out.reshape(100, 100)\n",
    "\n",
    "print(f\"Tensor shape: {data_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min original: 0.0, Max original: 6283.77294921875\n",
      "Min norm: 0.0, Max norm: 1.0\n"
     ]
    }
   ],
   "source": [
    "# --- NORMALIZACIÓN MIN-MAX [0, 1] ---\n",
    "mmn_min = data_tensor.min()\n",
    "mmn_max = data_tensor.max()\n",
    "\n",
    "print(f\"Min original: {mmn_min}, Max original: {mmn_max}\")\n",
    "\n",
    "data_norm = (data_tensor - mmn_min) / (mmn_max - mmn_min)\n",
    "\n",
    "print(f\"Min norm: {data_norm.min()}, Max norm: {data_norm.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Construcción de Entradas (Dependencias Temporales)\n",
    "El modelo captura dos tipos de dependencias temporales:\n",
    "\n",
    "1.  **Cercanía (Closeness - $X_c$)**: Captura la tendencia reciente. Se toman los últimos $l_c$ intervalos de tiempo.\n",
    "    $$ X_c = [X_{t-l_c}, X_{t-(l_c-1)}, \\dots, X_{t-1}] $$\n",
    "    \n",
    "2.  **Periodo (Period - $X_d$)**: Captura la periodicidad diaria (mismo hora del día en días anteriores). Se toman $l_p$ días.\n",
    "    $$ X_d = [X_{t-l_p \\cdot 24}, X_{t-(l_p-1) \\cdot 24}, \\dots, X_{t-24}] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando tensores Xc, Xd, Y...\n",
      "Generando dataset (Ventana Deslizante)...\n",
      "XC shape: (1416, 6, 100, 100)\n",
      "XD shape: (1416, 6, 100, 100)\n",
      "Y shape:  (1416, 2, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# --- GENERACIÓN DE DATASET (Ventana Deslizante) ---\n",
    "\n",
    "def create_dataset(data, len_c, len_p):\n",
    "    # data: [T, C, H, W]\n",
    "    T, C, H, W = data.shape\n",
    "    \n",
    "    XC, XD, Y = [], [], []\n",
    "    \n",
    "    # Empezamos después de tener suficiente historia para Periodo (que es el más largo)\n",
    "    # Periodo necesita: t - len_p * 24\n",
    "    start_idx = len_p * 24\n",
    "    \n",
    "    print(\"Generando dataset (Ventana Deslizante)...\")\n",
    "    for i in range(start_idx, T):\n",
    "        # Target\n",
    "        y = data[i] # [C, H, W]\n",
    "        \n",
    "        # Proximidad (Close): últimos len_c horas\n",
    "        # [i-len_c, ..., i-1]\n",
    "        xc = data[i - len_c : i] # [len_c, C, H, W]\n",
    "        # Concatenar en canales: [len_c * C, H, W]\n",
    "        xc = np.concatenate(xc, axis=0)\n",
    "        \n",
    "        # Periodo (Period): misma hora días anteriores\n",
    "        # [i - len_p*24, ..., i - 24]\n",
    "        xd = []\n",
    "        for p in range(len_p, 0, -1):\n",
    "            idx = i - p * 24\n",
    "            xd.append(data[idx])\n",
    "        xd = np.concatenate(xd, axis=0) # [len_p * C, H, W]\n",
    "        \n",
    "        XC.append(xc)\n",
    "        XD.append(xd)\n",
    "        Y.append(y)\n",
    "        \n",
    "    return np.array(XC), np.array(XD), np.array(Y)\n",
    "\n",
    "print(\"Generando tensores Xc, Xd, Y...\")\n",
    "XC, XD, Y = create_dataset(data_norm, LEN_CLOSE, LEN_PERIOD)\n",
    "print(f\"XC shape: {XC.shape}\")\n",
    "print(f\"XD shape: {XD.shape}\")\n",
    "print(f\"Y shape:  {Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1248\n",
      "Test samples:  168\n"
     ]
    }
   ],
   "source": [
    "# --- DIVISIÓN TRAIN / TEST ---\n",
    "# Test = Últimos 7 días (7 * 24 horas = 168 horas)\n",
    "test_hours = 7 * 24\n",
    "total_samples = len(Y)\n",
    "train_samples = total_samples - test_hours\n",
    "\n",
    "XC_train, XD_train, Y_train = XC[:train_samples], XD[:train_samples], Y[:train_samples]\n",
    "XC_test, XD_test, Y_test = XC[train_samples:], XD[train_samples:], Y[train_samples:]\n",
    "\n",
    "print(f\"Train samples: {len(Y_train)}\")\n",
    "print(f\"Test samples:  {len(Y_test)}\")\n",
    "\n",
    "# Convertir a Tensores PyTorch\n",
    "train_data = TensorDataset(torch.from_numpy(XC_train).float(), torch.from_numpy(XD_train).float(), torch.from_numpy(Y_train).float())\n",
    "test_data = TensorDataset(torch.from_numpy(XC_test).float(), torch.from_numpy(XD_test).float(), torch.from_numpy(Y_test).float())\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Arquitectura del Modelo (ST-DenseNet)\n",
    "\n",
    "El modelo utiliza dos ramas de redes convolucionales idénticas (una para $X_c$ y otra para $X_d$) que luego se fusionan. La característica principal es el uso de **Bloques Densos**.\n",
    "\n",
    "### 3.1. Unidad Densa (Dense Unit)\n",
    "En una red densa, la salida de cada capa se conecta a todas las capas siguientes. Para una capa $l$, la entrada es la concatenación de las salidas de todas las capas anteriores:\n",
    "\n",
    "$$ x_l = H_l([x_0, x_1, \\dots, x_{l-1}]) $$\n",
    "\n",
    "Donde $H_l(\\cdot)$ es una función compuesta por tres operaciones consecutivas:\n",
    "1.  Batch Normalization ($BN$)\n",
    "2.  Rectified Linear Unit ($ReLU$)\n",
    "3.  Convolución $3 \\times 3$ ($Conv$)\n",
    "\n",
    "$$ H_l(x) = Conv(ReLU(BN(x))) $$\n",
    "\n",
    "### 3.2. Ramas de Procesamiento\n",
    "Cada rama (Cercanía y Periodo) procesa su entrada a través de:\n",
    "1.  **Convolución Inicial**: Transforma la entrada al espacio de características.\n",
    "2.  **Bloque Denso**: Secuencia de $L$ Unidades Densas.\n",
    "3.  **Convolución Final**: Reduce la profundidad de los canales de vuelta a $N_{flow}$ (2 canales: SMS in/out).\n",
    "\n",
    "Sea $f_c$ la transformación de la rama de cercanía y $f_d$ la de la rama de periodo:\n",
    "$$ Y_c = f_c(X_c) $$\n",
    "$$ Y_d = f_d(X_d) $$\n",
    "\n",
    "### 3.3. Fusión Paramétrica Basada en Matrices\n",
    "Las salidas de ambas ramas se combinan utilizando una fusión ponderada por matrices de pesos aprendibles ($W_c$ y $W_d$). A diferencia de una suma simple, esto permite dar diferente importancia a la cercanía o al periodo para cada ubicación espacial (cada celda de la rejilla).\n",
    "\n",
    "$$ Y_{fusion} = W_c \\circ Y_c + W_d \\circ Y_d $$\n",
    "\n",
    "Donde:\n",
    "*   $\\circ$ denota el producto de Hadamard (multiplicación elemento a elemento).\n",
    "*   $W_c, W_d \\in \\mathbb{R}^{2 \\times 100 \\times 100}$ son parámetros aprendibles.\n",
    "\n",
    "Finalmente, se aplica una función de activación **Sigmoid** para asegurar que la salida esté en el rango $[0, 1]$ (consistente con la normalización Min-Max).\n",
    "\n",
    "$$ \\hat{X}_t = \\sigma(Y_{fusion}) = \\frac{1}{1 + e^{-Y_{fusion}}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=32):\n",
    "        super(DenseUnit, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(in_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv(out)\n",
    "        # Concatenar entrada y salida (Conexión Densa)\n",
    "        return torch.cat([x, out], 1)\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, in_channels, num_layers=3, growth_rate=32):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_channels = in_channels\n",
    "        for _ in range(num_layers):\n",
    "            self.layers.append(DenseUnit(current_channels, growth_rate))\n",
    "            current_channels += growth_rate\n",
    "            \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class STDenseNet(nn.Module):\n",
    "    def __init__(self, len_c, len_p, nb_flow, growth_rate=32):\n",
    "        super(STDenseNet, self).__init__()\n",
    "        \n",
    "        # Input channels for each branch\n",
    "        c_in = len_c * nb_flow\n",
    "        p_in = len_p * nb_flow\n",
    "        \n",
    "        # --- Branch Proximity (Close) ---\n",
    "        # Entrada -> Conv -> DenseBlock -> Conv\n",
    "        self.c_conv1 = nn.Conv2d(c_in, growth_rate, kernel_size=3, padding=1)\n",
    "        self.c_dense = DenseBlock(growth_rate, num_layers=3, growth_rate=growth_rate)\n",
    "        # Output channels after dense block: growth_rate + 3 * growth_rate = 4 * 32 = 128\n",
    "        dense_out_c = growth_rate + 3 * growth_rate\n",
    "        self.c_conv2 = nn.Conv2d(dense_out_c, nb_flow, kernel_size=3, padding=1)\n",
    "        \n",
    "        # --- Branch Period ---\n",
    "        self.p_conv1 = nn.Conv2d(p_in, growth_rate, kernel_size=3, padding=1)\n",
    "        self.p_dense = DenseBlock(growth_rate, num_layers=3, growth_rate=growth_rate)\n",
    "        self.p_conv2 = nn.Conv2d(dense_out_c, nb_flow, kernel_size=3, padding=1)\n",
    "        \n",
    "        # --- Fusion ---\n",
    "        # Parametric Matrix Fusion: Wc * Xc + Wd * Xd\n",
    "        # Wc, Wd are learnable parameters of shape (nb_flow, H, W)\n",
    "        self.Wc = nn.Parameter(torch.randn(nb_flow, H, W), requires_grad=True)\n",
    "        self.Wd = nn.Parameter(torch.randn(nb_flow, H, W), requires_grad=True)\n",
    "        \n",
    "        # Output Activation\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xc, xd):\n",
    "        # Proximity Branch\n",
    "        out_c = self.c_conv1(xc)\n",
    "        out_c = self.c_dense(out_c)\n",
    "        out_c = self.c_conv2(out_c)\n",
    "        \n",
    "        # Period Branch\n",
    "        out_p = self.p_conv1(xd)\n",
    "        out_p = self.p_dense(out_p)\n",
    "        out_p = self.p_conv2(out_p)\n",
    "        \n",
    "        # Fusion\n",
    "        fusion = out_c * self.Wc + out_p * self.Wd\n",
    "        \n",
    "        # Output\n",
    "        return self.sigmoid(fusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento\n",
    "\n",
    "### 4.1. Función de Pérdida (Loss Function)\n",
    "Se utiliza el Error Cuadrático Medio (MSE) entre la predicción y el valor real:\n",
    "\n",
    "$$ L(\\theta) = \\frac{1}{N} \\sum_{i=1}^{N} || \\hat{X}_t^{(i)} - X_t^{(i)} ||^2 $$\n",
    "\n",
    "Donde $\\theta$ son todos los parámetros aprendibles del modelo.\n",
    "\n",
    "### 4.2. Optimizador\n",
    "*   **Algoritmo**: Adam (Adaptive Moment Estimation).\n",
    "*   **Learning Rate**: Se utiliza un esquema de decaimiento (MultiStepLR) que reduce la tasa de aprendizaje en épocas específicas (50 y 75) para refinar la convergencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento en: cuda\n",
      "GPU Activa: NVIDIA GeForce RTX 5080\n",
      "Nota: Si el uso de GPU no sube inmediatamente, es porque se están cargando datos en RAM.\n"
     ]
    }
   ],
   "source": [
    "model = STDenseNet(LEN_CLOSE, LEN_PERIOD, NB_FLOW).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 75], gamma=0.1)\n",
    "\n",
    "print(f\"Iniciando entrenamiento en: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Activa: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"Nota: Si el uso de GPU no sube inmediatamente, es porque se están cargando datos en RAM.\")\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for xc, xd, y in train_loader:\n",
    "        xc, xd, y = xc.to(DEVICE), xd.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(xc, xd)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {avg_loss:.6f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.title('Curva de Aprendizaje')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretación de Resultados\n",
    "\n",
    "### 5.1. Métricas de Evaluación\n",
    "Para evaluar el rendimiento, primero se **desnormalizan** las predicciones para volver a la escala original de tráfico (número de SMS).\n",
    "\n",
    "$$ \\hat{X}_{orig} = \\hat{X}_{norm} \\cdot (Max - Min) + Min $$\n",
    "\n",
    "Se calculan las siguientes métricas:\n",
    "\n",
    "1.  **RMSE (Root Mean Squared Error)**: Error cuadrático medio. Penaliza más los errores grandes.\n",
    "    $$ RMSE = \\sqrt{\\frac{1}{M} \\sum (\\hat{x}_{orig} - x_{orig})^2} $$\n",
    "\n",
    "2.  **MAE (Mean Absolute Error)**: Error absoluto medio. Es más robusto a outliers y fácil de interpretar (error promedio en SMS).\n",
    "    $$ MAE = \\frac{1}{M} \\sum |\\hat{x}_{orig} - x_{orig}| $$\n",
    "\n",
    "3.  **R2 Score**: Coeficiente de determinación. Indica qué tan bien las predicciones se ajustan a los datos reales (1.0 es perfecto).\n",
    "\n",
    "### 5.2. Visualización\n",
    "*   **Scatter Plot**: Gráfico de dispersión de Predicción vs Realidad. Idealmente, los puntos deberían alinearse en la diagonal.\n",
    "*   **Distribución de Errores**: Histograma de los residuos. Debería estar centrado en 0.\n",
    "*   **Mapa de Calor de Error**: Muestra dónde se concentran los errores espacialmente.\n",
    "*   **Serie Temporal**: Comparación visual de la predicción y la realidad a lo largo del tiempo para una celda específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "ground_truth = []\n",
    "\n",
    "print(\"Iniciando evaluación...\")\n",
    "with torch.no_grad():\n",
    "    for xc, xd, y in test_loader:\n",
    "        xc, xd, y = xc.to(DEVICE), xd.to(DEVICE), y.to(DEVICE)\n",
    "        output = model(xc, xd)\n",
    "        predictions.append(output.cpu().numpy())\n",
    "        ground_truth.append(y.cpu().numpy())\n",
    "\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "ground_truth = np.concatenate(ground_truth, axis=0)\n",
    "\n",
    "# Rescalado Inverso (Denormalization)\n",
    "preds_rescaled = predictions * (mmn_max - mmn_min) + mmn_min\n",
    "gt_rescaled = ground_truth * (mmn_max - mmn_min) + mmn_min\n",
    "\n",
    "# --- MÉTRICAS ---\n",
    "mse = mean_squared_error(gt_rescaled.flatten(), preds_rescaled.flatten())\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(gt_rescaled.flatten(), preds_rescaled.flatten())\n",
    "r2 = r2_score(gt_rescaled.flatten(), preds_rescaled.flatten())\n",
    "\n",
    "print(f\"Resultados de Evaluación:\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"MAE:  {mae:.4f}\")\n",
    "print(f\"R2:   {r2:.4f}\")\n",
    "\n",
    "# --- VISUALIZACIONES ---\n",
    "\n",
    "# 1. Scatter Plot (Muestreado)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sample_idx = np.random.choice(gt_rescaled.flatten().shape[0], 10000, replace=False)\n",
    "plt.scatter(gt_rescaled.flatten()[sample_idx], preds_rescaled.flatten()[sample_idx], alpha=0.1, s=1)\n",
    "plt.plot([gt_rescaled.min(), gt_rescaled.max()], [gt_rescaled.min(), gt_rescaled.max()], 'r--')\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title('Ground Truth vs Prediction (Sampled)')\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribución de Errores\n",
    "errors = preds_rescaled.flatten() - gt_rescaled.flatten()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(errors, bins=100, log=True)\n",
    "plt.title('Distribución de Errores (Log Scale)')\n",
    "plt.xlabel('Error (Pred - GT)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.show()\n",
    "\n",
    "# 3. Mapa de Calor de Error Espacial (MAE)\n",
    "# Promedio sobre Tiempo y Canales\n",
    "mae_spatial = np.mean(np.abs(preds_rescaled - gt_rescaled), axis=(0, 1))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(mae_spatial, cmap='hot')\n",
    "plt.colorbar(label='MAE Promedio')\n",
    "plt.title('Mapa de Calor de Error Espacial (MAE)')\n",
    "plt.show()\n",
    "\n",
    "# 4. Serie Temporal para la Celda con más Tráfico\n",
    "total_traffic = np.sum(gt_rescaled, axis=(0, 1))\n",
    "y_max, x_max = np.unravel_index(np.argmax(total_traffic), total_traffic.shape)\n",
    "\n",
    "print(f\"Visualizando serie temporal para la celda con más tráfico: ({y_max}, {x_max})\")\n",
    "\n",
    "gt_series = gt_rescaled[:, 0, y_max, x_max] # Channel 0 (SMS In)\n",
    "pred_series = preds_rescaled[:, 0, y_max, x_max]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(gt_series, label='Ground Truth')\n",
    "plt.plot(pred_series, label='Prediction', alpha=0.7)\n",
    "plt.title(f'Serie Temporal de Tráfico SMS In - Celda ({y_max}, {x_max})')\n",
    "plt.xlabel('Horas (Test Set)')\n",
    "plt.ylabel('Volumen de SMS')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
